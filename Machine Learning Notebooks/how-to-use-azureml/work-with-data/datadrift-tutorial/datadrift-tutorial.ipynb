{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-quickdemo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyze data drift in Azure Machine Learning datasets \n",
        "\n",
        "In this tutorial, you will setup a data drift monitor on a weather dataset to:\n",
        "\n",
        "&#x2611; Analyze historical data for drift\n",
        "\n",
        "&#x2611; Setup a monitor to recieve email alerts if data drift is detected going forward\n",
        "\n",
        "If your workspace is Enterprise level, view and exlpore the results in the Azure Machine Learning studio. The video below shows the results from this tutorial. \n",
        "\n",
        "![gif](media/video.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "If you are using an Azure Machine Learning Compute instance, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) if you haven't already established your connection to the AzureML Workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SDK version: 1.38.0\n"
          ]
        }
      ],
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "\n",
        "print('SDK version:', azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Workspace\n",
        "\n",
        "Initialize a workspace object from persisted configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Workspace.create(name='lsmltraining', subscription_id='bfb59099-69db-4d2b-887e-abcf6ccdb5c4', resource_group='AZR-C11-DV-122-01-RnD')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config('./../../config_slaaddv01.json')\n",
        "ws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup target and baseline datasets\n",
        "\n",
        "Setup the baseline and target datasets. The baseline will be used to compare each time slice of the target dataset, which is sampled by a given frequency. For further details, see [our documentation](http://aka.ms/datadrift). \n",
        "\n",
        "The next few cells will:\n",
        "  * get the default datastore\n",
        "  * upload the `weather-data` to the datastore\n",
        "  * create the Tabular dataset from the data\n",
        "  * add the timeseries trait by specifying the timestamp column `datetime`\n",
        "  * register the dataset\n",
        "  * create the baseline as a time slice of the target dataset\n",
        "  * optionally, register the baseline dataset\n",
        "  \n",
        "The folder `weather-data` contains weather data from the [NOAA Integrated Surface Data](https://azure.microsoft.com/services/open-datasets/catalog/noaa-integrated-surface-data/) filtered down to to station names containing the string 'FLORIDA' to reduce the size of data. See `get_data.py` to see how this data is curated and modify as desired. This script may take a long time to run, hence the data is provided in the `weather-data` folder for this demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use default datastore\n",
        "dstore = ws.get_default_datastore()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading an estimated of 118 files\n",
            "Uploading weather-data\\2010\\11\\data.parquet\n",
            "Uploaded weather-data\\2010\\11\\data.parquet, 1 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\02\\data.parquet\n",
            "Uploaded weather-data\\2012\\02\\data.parquet, 2 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\12\\data.parquet\n",
            "Uploaded weather-data\\2010\\12\\data.parquet, 3 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\06\\data.parquet\n",
            "Uploaded weather-data\\2011\\06\\data.parquet, 4 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\01\\data.parquet\n",
            "Uploaded weather-data\\2011\\01\\data.parquet, 5 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\09\\data.parquet\n",
            "Uploaded weather-data\\2011\\09\\data.parquet, 6 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\11\\data.parquet\n",
            "Uploaded weather-data\\2011\\11\\data.parquet, 7 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\01\\data.parquet\n",
            "Uploaded weather-data\\2010\\01\\data.parquet, 8 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\02\\data.parquet\n",
            "Uploaded weather-data\\2010\\02\\data.parquet, 9 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\06\\data.parquet\n",
            "Uploaded weather-data\\2010\\06\\data.parquet, 10 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\08\\data.parquet\n",
            "Uploaded weather-data\\2011\\08\\data.parquet, 11 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\01\\data.parquet\n",
            "Uploaded weather-data\\2012\\01\\data.parquet, 12 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\08\\data.parquet\n",
            "Uploaded weather-data\\2012\\08\\data.parquet, 13 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\11\\data.parquet\n",
            "Uploaded weather-data\\2012\\11\\data.parquet, 14 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\06\\data.parquet\n",
            "Uploaded weather-data\\2014\\06\\data.parquet, 15 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\07\\data.parquet\n",
            "Uploaded weather-data\\2010\\07\\data.parquet, 16 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\05\\data.parquet\n",
            "Uploaded weather-data\\2011\\05\\data.parquet, 17 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\07\\data.parquet\n",
            "Uploaded weather-data\\2011\\07\\data.parquet, 18 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\06\\data.parquet\n",
            "Uploaded weather-data\\2012\\06\\data.parquet, 19 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\10\\data.parquet\n",
            "Uploaded weather-data\\2012\\10\\data.parquet, 20 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\12\\data.parquet\n",
            "Uploaded weather-data\\2012\\12\\data.parquet, 21 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\04\\data.parquet\n",
            "Uploaded weather-data\\2013\\04\\data.parquet, 22 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\09\\data.parquet\n",
            "Uploaded weather-data\\2014\\09\\data.parquet, 23 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\04\\data.parquet\n",
            "Uploaded weather-data\\2010\\04\\data.parquet, 24 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\09\\data.parquet\n",
            "Uploaded weather-data\\2010\\09\\data.parquet, 25 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\04\\data.parquet\n",
            "Uploaded weather-data\\2011\\04\\data.parquet, 26 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\10\\data.parquet\n",
            "Uploaded weather-data\\2011\\10\\data.parquet, 27 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\05\\data.parquet\n",
            "Uploaded weather-data\\2012\\05\\data.parquet, 28 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\03\\data.parquet\n",
            "Uploaded weather-data\\2013\\03\\data.parquet, 29 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\10\\data.parquet\n",
            "Uploaded weather-data\\2014\\10\\data.parquet, 30 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\12\\data.parquet\n",
            "Uploaded weather-data\\2014\\12\\data.parquet, 31 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\04\\data.parquet\n",
            "Uploaded weather-data\\2015\\04\\data.parquet, 32 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\05\\data.parquet\n",
            "Uploaded weather-data\\2010\\05\\data.parquet, 33 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\12\\data.parquet\n",
            "Uploaded weather-data\\2011\\12\\data.parquet, 34 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\01\\data.parquet\n",
            "Uploaded weather-data\\2013\\01\\data.parquet, 35 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\04\\data.parquet\n",
            "Uploaded weather-data\\2014\\04\\data.parquet, 36 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\11\\data.parquet\n",
            "Uploaded weather-data\\2014\\11\\data.parquet, 37 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\03\\data.parquet\n",
            "Uploaded weather-data\\2010\\03\\data.parquet, 38 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\08\\data.parquet\n",
            "Uploaded weather-data\\2010\\08\\data.parquet, 39 files out of an estimated total of 118\n",
            "Uploading weather-data\\2010\\10\\data.parquet\n",
            "Uploaded weather-data\\2010\\10\\data.parquet, 40 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\02\\data.parquet\n",
            "Uploaded weather-data\\2011\\02\\data.parquet, 41 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\09\\data.parquet\n",
            "Uploaded weather-data\\2012\\09\\data.parquet, 42 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\05\\data.parquet\n",
            "Uploaded weather-data\\2014\\05\\data.parquet, 43 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\02\\data.parquet\n",
            "Uploaded weather-data\\2015\\02\\data.parquet, 44 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\08\\data.parquet\n",
            "Uploaded weather-data\\2013\\08\\data.parquet, 45 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\11\\data.parquet\n",
            "Uploaded weather-data\\2013\\11\\data.parquet, 46 files out of an estimated total of 118\n",
            "Uploading weather-data\\2011\\03\\data.parquet\n",
            "Uploaded weather-data\\2011\\03\\data.parquet, 47 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\10\\data.parquet\n",
            "Uploaded weather-data\\2013\\10\\data.parquet, 48 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\01\\data.parquet\n",
            "Uploaded weather-data\\2014\\01\\data.parquet, 49 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\03\\data.parquet\n",
            "Uploaded weather-data\\2012\\03\\data.parquet, 50 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\12\\data.parquet\n",
            "Uploaded weather-data\\2013\\12\\data.parquet, 51 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\07\\data.parquet\n",
            "Uploaded weather-data\\2015\\07\\data.parquet, 52 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\08\\data.parquet\n",
            "Uploaded weather-data\\2015\\08\\data.parquet, 53 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\09\\data.parquet\n",
            "Uploaded weather-data\\2015\\09\\data.parquet, 54 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\11\\data.parquet\n",
            "Uploaded weather-data\\2015\\11\\data.parquet, 55 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\04\\data.parquet\n",
            "Uploaded weather-data\\2016\\04\\data.parquet, 56 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\04\\data.parquet\n",
            "Uploaded weather-data\\2012\\04\\data.parquet, 57 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\01\\data.parquet\n",
            "Uploaded weather-data\\2015\\01\\data.parquet, 58 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\12\\data.parquet\n",
            "Uploaded weather-data\\2015\\12\\data.parquet, 59 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\02\\data.parquet\n",
            "Uploaded weather-data\\2016\\02\\data.parquet, 60 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\11\\data.parquet\n",
            "Uploaded weather-data\\2016\\11\\data.parquet, 61 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\01\\data.parquet\n",
            "Uploaded weather-data\\2017\\01\\data.parquet, 62 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\07\\data.parquet\n",
            "Uploaded weather-data\\2016\\07\\data.parquet, 63 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\09\\data.parquet\n",
            "Uploaded weather-data\\2016\\09\\data.parquet, 64 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\10\\data.parquet\n",
            "Uploaded weather-data\\2016\\10\\data.parquet, 65 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\07\\data.parquet\n",
            "Uploaded weather-data\\2017\\07\\data.parquet, 66 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\08\\data.parquet\n",
            "Uploaded weather-data\\2017\\08\\data.parquet, 67 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\03\\data.parquet\n",
            "Uploaded weather-data\\2015\\03\\data.parquet, 68 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\05\\data.parquet\n",
            "Uploaded weather-data\\2015\\05\\data.parquet, 69 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\06\\data.parquet\n",
            "Uploaded weather-data\\2015\\06\\data.parquet, 70 files out of an estimated total of 118\n",
            "Uploading weather-data\\2015\\10\\data.parquet\n",
            "Uploaded weather-data\\2015\\10\\data.parquet, 71 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\01\\data.parquet\n",
            "Uploaded weather-data\\2016\\01\\data.parquet, 72 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\03\\data.parquet\n",
            "Uploaded weather-data\\2016\\03\\data.parquet, 73 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\06\\data.parquet\n",
            "Uploaded weather-data\\2016\\06\\data.parquet, 74 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\09\\data.parquet\n",
            "Uploaded weather-data\\2017\\09\\data.parquet, 75 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\11\\data.parquet\n",
            "Uploaded weather-data\\2017\\11\\data.parquet, 76 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\02\\data.parquet\n",
            "Uploaded weather-data\\2013\\02\\data.parquet, 77 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\09\\data.parquet\n",
            "Uploaded weather-data\\2013\\09\\data.parquet, 78 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\05\\data.parquet\n",
            "Uploaded weather-data\\2016\\05\\data.parquet, 79 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\08\\data.parquet\n",
            "Uploaded weather-data\\2016\\08\\data.parquet, 80 files out of an estimated total of 118\n",
            "Uploading weather-data\\2016\\12\\data.parquet\n",
            "Uploaded weather-data\\2016\\12\\data.parquet, 81 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\04\\data.parquet\n",
            "Uploaded weather-data\\2017\\04\\data.parquet, 82 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\02\\data.parquet\n",
            "Uploaded weather-data\\2018\\02\\data.parquet, 83 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\04\\data.parquet\n",
            "Uploaded weather-data\\2018\\04\\data.parquet, 84 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\09\\data.parquet\n",
            "Uploaded weather-data\\2018\\09\\data.parquet, 85 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\10\\data.parquet\n",
            "Uploaded weather-data\\2019\\10\\data.parquet, 86 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\02\\data.parquet\n",
            "Uploaded weather-data\\2014\\02\\data.parquet, 87 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\03\\data.parquet\n",
            "Uploaded weather-data\\2014\\03\\data.parquet, 88 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\07\\data.parquet\n",
            "Uploaded weather-data\\2014\\07\\data.parquet, 89 files out of an estimated total of 118\n",
            "Uploading weather-data\\2014\\08\\data.parquet\n",
            "Uploaded weather-data\\2014\\08\\data.parquet, 90 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\02\\data.parquet\n",
            "Uploaded weather-data\\2017\\02\\data.parquet, 91 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\03\\data.parquet\n",
            "Uploaded weather-data\\2017\\03\\data.parquet, 92 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\05\\data.parquet\n",
            "Uploaded weather-data\\2017\\05\\data.parquet, 93 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\06\\data.parquet\n",
            "Uploaded weather-data\\2017\\06\\data.parquet, 94 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\10\\data.parquet\n",
            "Uploaded weather-data\\2017\\10\\data.parquet, 95 files out of an estimated total of 118\n",
            "Uploading weather-data\\2017\\12\\data.parquet\n",
            "Uploaded weather-data\\2017\\12\\data.parquet, 96 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\01\\data.parquet\n",
            "Uploaded weather-data\\2018\\01\\data.parquet, 97 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\03\\data.parquet\n",
            "Uploaded weather-data\\2018\\03\\data.parquet, 98 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\08\\data.parquet\n",
            "Uploaded weather-data\\2018\\08\\data.parquet, 99 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\11\\data.parquet\n",
            "Uploaded weather-data\\2018\\11\\data.parquet, 100 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\12\\data.parquet\n",
            "Uploaded weather-data\\2018\\12\\data.parquet, 101 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\01\\data.parquet\n",
            "Uploaded weather-data\\2019\\01\\data.parquet, 102 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\02\\data.parquet\n",
            "Uploaded weather-data\\2019\\02\\data.parquet, 103 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\03\\data.parquet\n",
            "Uploaded weather-data\\2019\\03\\data.parquet, 104 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\05\\data.parquet\n",
            "Uploaded weather-data\\2019\\05\\data.parquet, 105 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\06\\data.parquet\n",
            "Uploaded weather-data\\2019\\06\\data.parquet, 106 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\07\\data.parquet\n",
            "Uploaded weather-data\\2019\\07\\data.parquet, 107 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\08\\data.parquet\n",
            "Uploaded weather-data\\2019\\08\\data.parquet, 108 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\09\\data.parquet\n",
            "Uploaded weather-data\\2019\\09\\data.parquet, 109 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\07\\data.parquet\n",
            "Uploaded weather-data\\2013\\07\\data.parquet, 110 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\05\\data.parquet\n",
            "Uploaded weather-data\\2018\\05\\data.parquet, 111 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\06\\data.parquet\n",
            "Uploaded weather-data\\2018\\06\\data.parquet, 112 files out of an estimated total of 118\n",
            "Uploading weather-data\\2019\\04\\data.parquet\n",
            "Uploaded weather-data\\2019\\04\\data.parquet, 113 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\06\\data.parquet\n",
            "Uploaded weather-data\\2013\\06\\data.parquet, 114 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\07\\data.parquet\n",
            "Uploaded weather-data\\2018\\07\\data.parquet, 115 files out of an estimated total of 118\n",
            "Uploading weather-data\\2018\\10\\data.parquet\n",
            "Uploaded weather-data\\2018\\10\\data.parquet, 116 files out of an estimated total of 118\n",
            "Uploading weather-data\\2012\\07\\data.parquet\n",
            "Uploaded weather-data\\2012\\07\\data.parquet, 117 files out of an estimated total of 118\n",
            "Uploading weather-data\\2013\\05\\data.parquet\n",
            "Uploaded weather-data\\2013\\05\\data.parquet, 118 files out of an estimated total of 118\n",
            "Uploaded 118 files\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "$AZUREML_DATAREFERENCE_c3bc7107f36744309fd6e818e01b8a9a"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# upload weather data\n",
        "dstore.upload('weather-data', 'datadrift-data', overwrite=True, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.experiment import Experiment\n",
        "experiment = Experiment(workspace=ws, name='exp-oakville-tire')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset_profile\n",
            "automl-diabetis\n",
            "mslearn-diabetes-automl-sdk\n",
            "mslearn-diabetes\n",
            "mslearn-train-diabetes\n",
            "mslearn-train-diabetes-2\n",
            "liverpool\n",
            "leicester\n",
            "liverpool-train-diabetes\n",
            "liverpool-diabetes-pipeline\n",
            "liverpool-diabetes-batch\n",
            "liverpool-diabetes-hyperdrive\n",
            "liverpool-diabetes-explain\n",
            "liverpool-diabetes-fairness\n",
            "liverpool-diabates-drift-Monitor-Runs\n",
            "diabetis-prediction-designer\n",
            "database-prediction-endpoint\n",
            "diabetis-data-pipeline-apr-10\n",
            "diabetis-data-pipeline-apr-10a\n",
            "LabelingExperiment\n",
            "exp-oakville-tire\n"
          ]
        }
      ],
      "source": [
        "list_experiments = Experiment.list(ws)\n",
        "for exp in list_experiments:\n",
        "    print(exp.name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target_weather_data\n",
            "animal-images-dataset\n",
            "TD-Diabetis-Data-Pipeline-02-Normalize_Data-Transformation_function-07b0ba18\n",
            "MD-Diabetis-Data-Pipeline-02-Train_Model-Trained_model-021accfd\n",
            "TD-Diabetis-Data-Pipeline-01-Normalize_Data-Transformation_function-07b0ba18\n",
            "MD-Diabetis-Data-Pipeline-01-Train_Model-Trained_model-021accfd\n",
            "diabetes target\n",
            "diabetes baseline\n",
            "ds_liverpool diabetes\n",
            "ds_liverpool\n",
            "batch-data\n",
            "liverpool_ds\n",
            "diabetes file dataset\n",
            "diabetes dataset\n",
            "diabetis-dataset\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.dataset import Dataset\n",
        "list_reg_ds = Dataset.get_all(ws)\n",
        "for ds in list_reg_ds:\n",
        "    print(ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import Dataset class\n",
        "from azureml.core import Dataset\n",
        "\n",
        "# create target dataset \n",
        "target = Dataset.Tabular.from_parquet_files(dstore.path('datadrift-data/**/data.parquet'))\n",
        "# set the timestamp column\n",
        "target = target.with_timestamp_columns('datetime')\n",
        "# register the target dataset\n",
        "target = target.register(ws, 'target_weather_data')\n",
        "# retrieve the dataset from the workspace by name\n",
        "target = Dataset.get_by_name(ws, 'target_weather_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import datetime \n",
        "from datetime import datetime\n",
        "\n",
        "# set baseline dataset as January 2019 weather data\n",
        "baseline = Dataset.Tabular.from_parquet_files(dstore.path('datadrift-data/2019/01/data.parquet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optionally, register the baseline dataset. if skipped, an unregistered dataset will be used\n",
        "baseline = baseline.register(ws, 'baseline_weather_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline dataset #recs: 4048\n",
            "Target dataset #recs: 434386\n"
          ]
        }
      ],
      "source": [
        "print(\"Baseline dataset #recs:\",len(baseline.to_pandas_dataframe()))\n",
        "print(\"Target dataset #recs:\",len(target.to_pandas_dataframe()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create compute target\n",
        "\n",
        "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
        "\n",
        "Create an Azure Machine Learning compute cluster to run the data drift monitor and associated runs. The below cell will create a compute cluster named `'cpu-cluster'`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "creating a new compute target...\n",
            "InProgress.\n",
            "SucceededProvisioning operation finished, operation \"Succeeded\"\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n",
            "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Resizing', 'allocationStateTransitionTime': '2022-04-14T18:24:37.955000+00:00', 'errors': None, 'creationTime': '2022-04-14T18:24:37.266407+00:00', 'modifiedTime': '2022-04-14T18:24:41.157142+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 2, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D3_V2'}\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.compute import AmlCompute, ComputeTarget\n",
        "\n",
        "compute_name = 'leicestercluster'\n",
        "\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print('found compute target. just use it. ' + compute_name)\n",
        "else:\n",
        "    print('creating a new compute target...')\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2', min_nodes=0, max_nodes=2)\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "\n",
        "    # can poll for a minimum number of nodes and for a specific timeout.\n",
        "    # if no min node count is provided it will use the scale settings for the cluster\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "\n",
        "    # For a more detailed view of current AmlCompute status, use get_status()\n",
        "    print(compute_target.get_status().serialize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create data drift monitor\n",
        "\n",
        "See [our documentation](http://aka.ms/datadrift) for a complete description for all of the parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "tags": [
          "datadrift-remarks-sample"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "from azureml.datadrift import DataDriftDetector, AlertConfiguration\n",
        "\n",
        "alert_config = AlertConfiguration(['anton.georgescu@finastra.com']) # replace with your email to recieve alerts from the scheduled pipeline after enabling\n",
        "\n",
        "monitor = DataDriftDetector.create_from_datasets(ws, 'weather-monitor-2', baseline, target, \n",
        "                                                      compute_target='leicestercluster',    # compute target for scheduled pipeline and backfills \n",
        "                                                      frequency='Week',                     # how often to analyze target data\n",
        "                                                      feature_list=None,                    # list of features to detect drift on\n",
        "                                                      drift_threshold=None,                 # threshold from 0 to 1 for email alerting\n",
        "                                                      latency=0,                            # SLA in hours for target data to arrive in the dataset\n",
        "                                                      alert_config=alert_config)            # email addresses to send alert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Update data drift monitor\n",
        "\n",
        "Many settings of the data drift monitor can be updated after creation. In this demo, we will update the `drift_threshold` and `feature_list`. See [our documentation](http://aka.ms/datadrift) for details on which settings can be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usaf\n",
            "wban\n",
            "datetime\n",
            "latitude\n",
            "longitude\n",
            "elevation\n",
            "windAngle\n",
            "windSpeed\n",
            "temperature\n",
            "seaLvlPressure\n",
            "cloudCoverage\n",
            "presentWeatherIndicator\n",
            "pastWeatherIndicator\n",
            "precipTime\n",
            "precipDepth\n",
            "snowDepth\n",
            "stationName\n",
            "countryOrRegion\n",
            "p_k\n",
            "Current state: Disabled\n",
            "2022-04-14 14:25:59,642 - azureml.datadrift._logging._telemetry_logger.azureml.datadrift.datadriftdetector - WARNING - Alert has not been setup. Datadriftdetector with id: 2cbf0636-53f9-4e84-992b-d551b120a3a8.\n",
            "This may be because you do not have access to the AppInsights associated with this AzureML Workspace - activity_id:2f60997c-d879-45d8-97a2-20d440702a75 activity_name:update activity_type:InternalCall tenant_id:None subscription_id:bfb59099-69db-4d2b-887e-abcf6ccdb5c4 resource_group:AZR-C11-DV-122-01-RnD workspace_id:f5d4ea3c-2017-4bfd-9657-00b6ab26ff8c workspace_location:canadacentral compute_type:None compute_size:None compute_nodes_min:None compute_nodes_max:None image_id:None dd_id:2cbf0636-53f9-4e84-992b-d551b120a3a8 dd_type:DatasetBased freq:Week interval:1 scheduling:Disabled threshold:None latency:0 total_features:0 services:None train_dataset_id:None baseline_dataset_id:6cf240fc-935f-43f4-ace6-0ccef5d2e96a target_dataset_id:8942cb03-0aec-45f7-ad0b-40a9d4a04312 log_env:sdk client telemetry_event_id:e8aaf383-21ad-4156-bc40-6c12eda88cc1 new_total_features:19 sdk_version:1.40.0 telemetry_component_name:azureml.datadrift\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Program Files\\Python37\\lib\\site-packages\\azureml\\datadrift\\datadriftdetector.py:1371: UserWarning: Alert has not been setup. Datadriftdetector with id: 2cbf0636-53f9-4e84-992b-d551b120a3a8.\n",
            "This may be because you do not have access to the AppInsights associated with this AzureML Workspace\n",
            "  warnings.warn(error_msg)\n"
          ]
        }
      ],
      "source": [
        "# get monitor by name\n",
        "monitor = DataDriftDetector.get_by_name(ws, 'weather-monitor-2')\n",
        "\n",
        "# create feature list - need to exclude columns that naturally drift or increment over time, such as year, day, index\n",
        "columns  = list(baseline.take(1).to_pandas_dataframe())\n",
        "exclude  = ['year', 'day', 'version', '__index_level_0__']\n",
        "features = [col for col in columns if col not in exclude]\n",
        "for f in features:\n",
        "    print(f)\n",
        "\n",
        "# update the feature list\n",
        "monitor  = monitor.update(feature_list=features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze historical data and backfill\n",
        "\n",
        "You can use the `backfill` method to:\n",
        "  * analyze historical data\n",
        "  * backfill metrics after updating the settings (mainly the feature list)\n",
        "  * backfill metrics for failed runs\n",
        "  \n",
        "The below cells will run two backfills that will produce data drift results for 2019 weather data, with January used as the baseline in the monitor. The output can be seen from the `show` method after the runs have completed, or viewed from the Azure Machine Learning studio for Enterprise workspaces.\n",
        "\n",
        "![Drift results](media/drift-results.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "source": [
        ">**Tip!** When starting with the data drift capability, start by backfilling on a small section of data to get initial results. Update the feature list as needed by removing columns that are causing drift, but can be ignored, and backfill this section of data until satisfied with the results. Then, backfill on a larger slice of data and/or set the alert configuration, threshold, and enable the schedule to recieve alerts to drift on your dataset. All of this can be done through the UI (Enterprise) or Python SDK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although it depends on many factors, the below backfill should typically take less than 20 minutes to run. Results will show as soon as they become available, not when the backfill is completed, so you may begin to see some metrics in a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>weather-monitor-2-Monitor-Runs</td><td>weather-monitor-2-Monitor-Runs_1649960878436</td><td>DatasetDriftRunBackFill</td><td>Queued</td><td><a href=\"https://ml.azure.com/runs/weather-monitor-2-Monitor-Runs_1649960878436?wsid=/subscriptions/bfb59099-69db-4d2b-887e-abcf6ccdb5c4/resourcegroups/AZR-C11-DV-122-01-RnD/workspaces/lsmltraining&amp;tid=f6b81657-e1f0-4671-824b-30d72a53b469\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.Run?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
            ],
            "text/plain": [
              "Run(Experiment: weather-monitor-2-Monitor-Runs,\n",
              "Id: weather-monitor-2-Monitor-Runs_1649960878436,\n",
              "Type: DatasetDriftRunBackFill,\n",
              "Status: Queued)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# backfill for one month\n",
        "backfill_start_date = datetime(2019, 9, 1)\n",
        "backfill_end_date = datetime(2019, 10, 1)\n",
        "backfill = monitor.backfill(backfill_start_date, backfill_end_date)\n",
        "backfill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query metrics and show results in Python\n",
        "\n",
        "The below cell will plot some key data drift metrics, and can be used to query the results. Run `help(monitor.get_output)` for specifics on the object returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'runId': 'weather-monitor-2-Monitor-Runs_1649960878436',\n",
              " 'target': 'leicestercluster',\n",
              " 'status': 'Canceled',\n",
              " 'startTimeUtc': '2022-04-14T18:29:08.540989Z',\n",
              " 'endTimeUtc': '2022-04-14T18:29:08.800918Z',\n",
              " 'services': {},\n",
              " 'properties': {'_azureml.ComputeTargetType': 'amlctrain',\n",
              "  'ContentSnapshotId': '0d586690-8d5d-4962-8b4b-332e72721d44',\n",
              "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
              "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
              " 'inputDatasets': [],\n",
              " 'outputDatasets': [],\n",
              " 'runDefinition': {'script': '_generate_script_datasets.py',\n",
              "  'useAbsolutePath': False,\n",
              "  'arguments': ['--baseline_dataset_id',\n",
              "   '6cf240fc-935f-43f4-ace6-0ccef5d2e96a',\n",
              "   '--target_dataset_id',\n",
              "   '8942cb03-0aec-45f7-ad0b-40a9d4a04312',\n",
              "   '--workspace_name',\n",
              "   'lsmltraining',\n",
              "   '--workspace_location',\n",
              "   'canadacentral',\n",
              "   '--instrumentation_key',\n",
              "   'e17bd6de-8437-41d5-823b-6db68d4d87dc',\n",
              "   '--ai_endpoint',\n",
              "   'https://dc.applicationinsights.azure.com/v2/track',\n",
              "   '--subscription_id',\n",
              "   'bfb59099-69db-4d2b-887e-abcf6ccdb5c4',\n",
              "   '--enable_metric_logger',\n",
              "   'true',\n",
              "   '--run_type',\n",
              "   'BackFill',\n",
              "   '--drift_threshold',\n",
              "   '0',\n",
              "   '--datadrift_id',\n",
              "   '2cbf0636-53f9-4e84-992b-d551b120a3a8',\n",
              "   '--datadrift_run_id',\n",
              "   'c0820ded-45c1-4c5f-b38e-8d949323c6e2',\n",
              "   '--datadrift_name',\n",
              "   'weather-monitor-2',\n",
              "   '--frequency',\n",
              "   'Week',\n",
              "   '--datadrift_configuration_type',\n",
              "   'DatasetBased',\n",
              "   '--start_date',\n",
              "   '2019-09-01',\n",
              "   '--end_date',\n",
              "   '2019-10-06',\n",
              "   '--features_whitelist',\n",
              "   'usaf',\n",
              "   'wban',\n",
              "   'datetime',\n",
              "   'latitude',\n",
              "   'longitude',\n",
              "   'elevation',\n",
              "   'windAngle',\n",
              "   'windSpeed',\n",
              "   'temperature',\n",
              "   'seaLvlPressure',\n",
              "   'cloudCoverage',\n",
              "   'presentWeatherIndicator',\n",
              "   'pastWeatherIndicator',\n",
              "   'precipTime',\n",
              "   'precipDepth',\n",
              "   'snowDepth',\n",
              "   'stationName',\n",
              "   'countryOrRegion',\n",
              "   'p_k'],\n",
              "  'sourceDirectoryDataStore': None,\n",
              "  'framework': 'Python',\n",
              "  'communicator': 'None',\n",
              "  'target': 'leicestercluster',\n",
              "  'dataReferences': {},\n",
              "  'data': {},\n",
              "  'outputData': {},\n",
              "  'datacaches': [],\n",
              "  'jobName': None,\n",
              "  'maxRunDurationSeconds': None,\n",
              "  'nodeCount': 1,\n",
              "  'instanceTypes': [],\n",
              "  'priority': None,\n",
              "  'credentialPassthrough': False,\n",
              "  'identity': None,\n",
              "  'environment': {'name': 'Experiment weather-monitor-2-Monitor-Runs Environment',\n",
              "   'version': 'Autosave_2022-04-14T18:26:04Z_6e7e871c',\n",
              "   'python': {'interpreterPath': 'python',\n",
              "    'userManagedDependencies': False,\n",
              "    'condaDependencies': {'dependencies': ['python=3.6.2',\n",
              "      'scikit-learn',\n",
              "      'scipy>=1.0.0',\n",
              "      'numpy',\n",
              "      'lightgbm<=3.1.0',\n",
              "      'pandas',\n",
              "      'pyarrow>=0.11.0',\n",
              "      'jsonpickle',\n",
              "      'psutil',\n",
              "      {'pip': ['azureml-defaults==1.39.0', 'azureml-datadrift==1.39.0']}],\n",
              "     'name': 'azureml_3831db1fc738414308f3acfc4e1b8141'},\n",
              "    'baseCondaEnvironment': None},\n",
              "   'environmentVariables': {},\n",
              "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04',\n",
              "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
              "    'baseDockerfile': None,\n",
              "    'baseImageRegistry': {'address': None,\n",
              "     'username': None,\n",
              "     'password': None}},\n",
              "   'spark': {'repositories': [], 'packages': [], 'precachePackages': True},\n",
              "   'inferencingStackVersion': None},\n",
              "  'history': {'outputCollection': True,\n",
              "   'directoriesToWatch': None,\n",
              "   'enableMLflowTracking': False},\n",
              "  'spark': {'configuration': {}},\n",
              "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
              "   'workerCountPerNode': 1,\n",
              "   'terminalExitCodes': None,\n",
              "   'configuration': {}},\n",
              "  'amlCompute': {'name': None,\n",
              "   'vmSize': None,\n",
              "   'retainCluster': False,\n",
              "   'clusterMaxNodeCount': 1},\n",
              "  'aiSuperComputer': {'instanceType': 'D2',\n",
              "   'imageVersion': 'pytorch-1.7.0',\n",
              "   'location': None,\n",
              "   'aiSuperComputerStorageData': None,\n",
              "   'interactive': False,\n",
              "   'scalePolicy': None,\n",
              "   'virtualClusterArmId': None,\n",
              "   'tensorboardLogDirectory': None,\n",
              "   'sshPublicKey': None,\n",
              "   'sshPublicKeys': None,\n",
              "   'enableAzmlInt': True,\n",
              "   'priority': 'Medium',\n",
              "   'slaTier': 'Standard',\n",
              "   'userAlias': None},\n",
              "  'kubernetesCompute': {'instanceType': None},\n",
              "  'tensorflow': {'workerCount': 0, 'parameterServerCount': 0},\n",
              "  'mpi': {'processCountPerNode': 0},\n",
              "  'pyTorch': {'communicationBackend': None, 'processCount': None},\n",
              "  'hdi': {'yarnDeployMode': 'None'},\n",
              "  'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5},\n",
              "  'exposedPorts': None,\n",
              "  'docker': {'useDocker': True,\n",
              "   'sharedVolumes': True,\n",
              "   'shmSize': '2g',\n",
              "   'arguments': []},\n",
              "  'cmk8sCompute': {'configuration': {}},\n",
              "  'commandReturnCodeConfig': {'returnCode': 'Zero',\n",
              "   'successfulReturnCodes': []},\n",
              "  'environmentVariables': {},\n",
              "  'applicationEndpoints': {},\n",
              "  'parameters': []},\n",
              " 'logFiles': {},\n",
              " 'submittedBy': 'Anton Georgescu'}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make sure the backfill has completed\n",
        "backfill.wait_for_completion(wait_post_processing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get results from Python SDK (wait for backfills or monitor runs to finish)\n",
        "results, metrics = monitor.get_output(start_time=datetime(year=2019, month=9, day=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the results from Python SDK \n",
        "monitor.show(backfill_start_date, backfill_end_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enable the monitor's pipeline schedule\n",
        "\n",
        "Turn on a scheduled pipeline which will anlayze the target dataset for drift every `frequency`. Use the latency parameter to adjust the start time of the pipeline. For instance, if it takes 24 hours for my data processing pipelines for data to arrive in the target dataset, set latency to 24. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# enable the pipeline schedule and recieve email alerts\n",
        "monitor.enable_schedule()\n",
        "\n",
        "# disable the pipeline schedule \n",
        "#monitor.disable_schedule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delete compute target\n",
        "\n",
        "Do not delete the compute target if you intend to keep using it for the data drift monitor scheduled runs or otherwise. If the minimum nodes are set to 0, it will scale down soon after jobs are completed, and scale up the next time the cluster is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# optionally delete the compute target\n",
        "#compute_target.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Delete the DataDriftDetector\n",
        "\n",
        "Invoking the `delete()` method on the object deletes the the drift monitor permanently and cannot be undone. You will no longer be able to find it in the UI and the `list()` or `get()` methods. The object on which delete() was called will have its state set to deleted and name suffixed with deleted. The baseline and target datasets and model data that was collected, if any, are not deleted. The compute is not deleted. The DataDrift schedule pipeline is disabled and archived."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "monitor.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "  * See [our documentation](https://aka.ms/datadrift) or [Python SDK reference](https://docs.microsoft.com/python/api/overview/azure/ml/intro)\n",
        "  * [Send requests or feedback](mailto:driftfeedback@microsoft.com) on data drift directly to the team\n",
        "  * Please open issues with data drift here on GitHub or on StackOverflow if others are likely to run into the same issue"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "jamgan"
      }
    ],
    "category": "tutorial",
    "compute": [
      "Remote"
    ],
    "datasets": [
      "NOAA"
    ],
    "deployment": [
      "None"
    ],
    "exclude_from_index": false,
    "framework": [
      "Azure ML"
    ],
    "friendly_name": "Data drift quickdemo",
    "index_order": 1,
    "interpreter": {
      "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
    },
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "star_tag": [
      "featured"
    ],
    "tags": [
      "Dataset",
      "Timeseries",
      "Drift"
    ],
    "task": "Filtering"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
